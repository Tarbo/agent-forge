# LLM Export Tools - Environment Configuration
# Copy this file to .env and configure your settings

# ============================================================================
# LLM PROVIDER (Choose ONE)
# ============================================================================

# Option 1: Ollama (Local, Private, No API Costs)
# Recommended: phi3_q4:latest, mistral:7b-instruct, or qwen2.5:7b-instruct
# IMPORTANT: Use "instruct" variant for instruction-following tasks
# Make sure Ollama is running: ollama serve
USE_OLLAMA=true
OLLAMA_MODEL=phi3_q4:latest
OLLAMA_BASE_URL=http://localhost:11434

# Option 2: OpenAI (Cloud, Requires API Key)
# OPENAI_API_KEY=sk-your-openai-key-here
# OPENAI_MODEL=gpt-3.5-turbo

# Option 3: Anthropic (Cloud, Requires API Key)
# ANTHROPIC_API_KEY=sk-ant-your-anthropic-key-here
# ANTHROPIC_MODEL=claude-3-5-sonnet-20241022

# ============================================================================
# APPLICATION SETTINGS
# ============================================================================

# Where to save exported files
EXPORT_DIRECTORY=~/Downloads/llm-exports

# Automatically open files after export (default: false)
# Set to 'true' if you want files to auto-open
# AUTO_OPEN_FILE=false

# ============================================================================
# NOTES
# ============================================================================

# For Ollama:
# 1. Install Ollama: https://ollama.ai
# 2. Download model: ollama pull phi3_q4 (or mistral:7b-instruct, qwen2.5:7b-instruct)
# 3. Start server: ollama serve
# 4. Or import GGUF from HuggingFace if behind corporate firewall (see README)

# Priority order:
# 1. Ollama (if USE_OLLAMA=true)
# 2. OpenAI (if OPENAI_API_KEY exists)
# 3. Anthropic (if ANTHROPIC_API_KEY exists)

