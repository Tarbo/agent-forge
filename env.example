# LLM Export Tools - Environment Configuration
# Copy this file to .env and configure your settings

# ============================================================================
# LLM PROVIDER (Choose ONE)
# ============================================================================

# Option 1: Ollama (Local, Private, No API Costs)
# Recommended model: llama3.2 (good balance of speed and quality)
# Make sure Ollama is running: ollama serve
USE_OLLAMA=true
OLLAMA_MODEL=llama3.2
OLLAMA_BASE_URL=http://localhost:11434

# Option 2: OpenAI (Cloud, Requires API Key)
# OPENAI_API_KEY=sk-your-openai-key-here
# OPENAI_MODEL=gpt-3.5-turbo

# Option 3: Anthropic (Cloud, Requires API Key)
# ANTHROPIC_API_KEY=sk-ant-your-anthropic-key-here
# ANTHROPIC_MODEL=claude-3-5-sonnet-20241022

# ============================================================================
# APPLICATION SETTINGS
# ============================================================================

# Where to save exported files
EXPORT_DIRECTORY=~/Downloads/llm-exports

# ============================================================================
# NOTES
# ============================================================================

# For Ollama:
# 1. Install Ollama: https://ollama.ai
# 2. Download model: ollama pull llama3.2
# 3. Start server: ollama serve
# 4. Or import from HuggingFace (see README for instructions)

# Priority order:
# 1. Ollama (if USE_OLLAMA=true)
# 2. OpenAI (if OPENAI_API_KEY exists)
# 3. Anthropic (if ANTHROPIC_API_KEY exists)

